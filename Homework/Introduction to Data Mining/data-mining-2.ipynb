{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train3.csv', 'label3.csv', 'train1.csv', 'train2.csv', 'test5.csv', 'test4.csv', 'test3.csv', 'train5.csv', 'test1.csv', 'label4.csv', 'label5.csv', 'label1.csv', 'label2.csv', 'train4.csv', 'test2.csv', 'test6.csv']\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.array([]).reshape([0, 13])\n",
    "# train_label = np.array([])\n",
    "# test_data = np.array([]).reshape([0, 13])\n",
    "\n",
    "# for i in range(1, 3):\n",
    "#     train_data = np.concatenate((train_data, pd.read_csv('../input/train%d.csv' % i, header=None).values))\n",
    "#     train_label = np.concatenate((train_label, pd.read_csv('../input/label%d.csv' % i, header=None).values.squeeze(-1)))\n",
    "# for i in range(1, 7):\n",
    "#     test_data = np.concatenate((test_data, pd.read_csv('../input/test%d.csv' % i, header=None).values))\n",
    "    \n",
    "# print(train_data.shape)\n",
    "# print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestRegressor(n_estimators=20,\n",
    "#                            n_jobs=-1,\n",
    "#                            max_features=6,\n",
    "#                            min_samples_split=10000,\n",
    "#                            min_samples_leaf=10,\n",
    "#                            max_depth=10)\n",
    "# model.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = model.predict(test_data)\n",
    "# df = pd.DataFrame({ 'id': range(1, arr.size + 1), 'Predicted': arr })\n",
    "# df.to_csv('./rfr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor(object):\n",
    "    \"\"\"RandomForestrRegressor\n",
    "    \"\"\"\n",
    "    def __init__(self, n_trees, sample_size, min_leaf_size=5, n_jobs=-1, max_depth=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.sample_size = sample_size\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.n_jobs = n_jobs\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        \n",
    "    def get_sample_items(self):\n",
    "        idxs = np.random.choice(len(self.data), self.sample_size)\n",
    "        return self.data.iloc[idxs], self.label[idxs]\n",
    "    \n",
    "    def create_trees(self): \n",
    "        return [DecisionTreeRegressor(self.min_leaf_size, self.max_depth) for i in range(self.n_trees)]\n",
    "\n",
    "    def fit(self, x, y, n_jobs=None):\n",
    "        if type(x) != pd.DataFrame:\n",
    "            x = pd.DataFrame(x)\n",
    "        self.data = x\n",
    "        self.label = y\n",
    "        if n_jobs == None:\n",
    "            n_jobs = self.n_jobs\n",
    "        self.fit_with_workers(self.fit_tree, n_jobs)\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        \n",
    "    def fit_with_workers(self, fit_fn, n_jobs=-1):\n",
    "        try:\n",
    "            workers = cpu_count()\n",
    "        except NotImplementedError:\n",
    "            workers = 1\n",
    "        if n_jobs != -1:\n",
    "            workers = n_jobs\n",
    "        pool = Pool(processes=workers)\n",
    "        print('fit with %d workers' % workers)\n",
    "        result = pool.map(fit_fn, self.create_trees())\n",
    "        self.trees += list(result)\n",
    "    \n",
    "    def fit_tree(self, tree):\n",
    "        # print('start fit_tree')\n",
    "        data, label = self.get_sample_items()\n",
    "        tree = tree.fit(data, label)\n",
    "        # print('stop fit_tree')\n",
    "        return tree\n",
    "        \n",
    "    def predict(self, x, n_jobs=None):\n",
    "        if n_jobs == None:\n",
    "            n_jobs = self.n_jobs\n",
    "        self.predict_data = x\n",
    "        predicted = np.stack(self.predict_with_workers(self.predict_tree, n_jobs))\n",
    "        all_predicted = np.mean(predicted, axis=0)\n",
    "        self.predict_data = None\n",
    "        return all_predicted\n",
    "    \n",
    "    def predict_with_workers(self, predict_fn, n_jobs=-1):\n",
    "        try:\n",
    "            workers = cpu_count()\n",
    "        except NotImplementedError:\n",
    "            workers = 1\n",
    "        if n_jobs != -1:\n",
    "            workers = n_jobs\n",
    "        pool = Pool(processes=workers)\n",
    "        print('predict with %d workers' % workers)\n",
    "        result = pool.map(predict_fn, self.trees)\n",
    "        return list(result)\n",
    "    \n",
    "    def predict_tree(self, tree):\n",
    "        # print('start predict_tree')\n",
    "        predicted = tree.predict(self.predict_data)\n",
    "        # print('stop predict_tree')\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(object):\n",
    "    \"\"\"DecisionTreeRegressor\n",
    "    \"\"\"\n",
    "    def __init__(self, min_leaf_size, max_depth=None):\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.max_depth = max_depth\n",
    "        self.split_idx = -1\n",
    "        self.split_value = None\n",
    "        self.value = None\n",
    "        self.l_tree = None\n",
    "        self.r_tree = None\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.value = np.mean(y)\n",
    "        if self.max_depth is not None and self.max_depth <= 1:\n",
    "            return self\n",
    "        self.split(x, y)\n",
    "        return self\n",
    "    \n",
    "    def split(self, data, label):\n",
    "        split_idx, split_value, score = self.find_split_point(data, label)\n",
    "        if score == float('inf'):\n",
    "            return \n",
    "        self.split_idx = split_idx\n",
    "        self.split_value = split_value\n",
    "        \n",
    "        l_idxs = np.nonzero(data.iloc[:, split_idx] <= split_value)[0]\n",
    "        r_idxs = np.nonzero(data.iloc[:, split_idx] > split_value)[0]\n",
    "        \n",
    "        self.l_tree = DecisionTreeRegressor(self.min_leaf_size, self.max_depth - 1)\n",
    "        self.r_tree = DecisionTreeRegressor(self.min_leaf_size, self.max_depth - 1)\n",
    "        \n",
    "        self.l_tree.fit(data.iloc[l_idxs], label[l_idxs])\n",
    "        self.r_tree.fit(data.iloc[r_idxs], label[r_idxs])\n",
    "            \n",
    "    def find_split_point(self, data, label):\n",
    "        sample_size = len(data)\n",
    "        best_score = float('inf')\n",
    "        best_split_idx = -1\n",
    "        best_split_value = None\n",
    "        for idx in range(len(data.columns)):\n",
    "            x = data.values[:, idx]\n",
    "            sorted_idxs = np.argsort(x)\n",
    "            x = x[sorted_idxs]\n",
    "            y =  label[sorted_idxs]\n",
    "            \n",
    "            y_sum = y.sum()\n",
    "            y_square_sum = (y ** 2).sum()\n",
    "            l_sample_size = 0\n",
    "            l_y_sum = 0.0\n",
    "            l_y_square_sum = 0.0\n",
    "            r_sample_size = sample_size\n",
    "            r_y_sum = y_sum\n",
    "            r_y_square_sum = y_square_sum\n",
    "\n",
    "            for i in range(sample_size - self.min_leaf_size):\n",
    "                r_sample_size -= 1\n",
    "                r_y_sum -= y[i]\n",
    "                r_y_square_sum -= (y[i] ** 2)\n",
    "                l_sample_size += 1\n",
    "                l_y_sum += y[i]\n",
    "                l_y_square_sum += (y[i] ** 2)\n",
    "\n",
    "                if i < self.min_leaf_size or x[i] == x[i+1]:\n",
    "                    continue\n",
    "\n",
    "                l_impurity = (l_y_square_sum / l_sample_size) - (l_y_sum / l_sample_size) ** 2\n",
    "                r_impurity = (r_y_square_sum / r_sample_size) - (r_y_sum / r_sample_size) ** 2\n",
    "                score = (l_impurity * l_sample_size + r_impurity * r_sample_size) / sample_size\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_split_value = x[i]\n",
    "                    best_split_idx = idx\n",
    "        return best_split_idx, best_split_value, best_score\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if type(x) == pd.DataFrame:\n",
    "            x = x.values\n",
    "        return np.array([self.predict_row(row) for row in x])\n",
    "    \n",
    "    def predict_row(self, row):\n",
    "        if self.l_tree == None:\n",
    "            return self.value\n",
    "        if row[self.split_idx] <= self.split_value:\n",
    "            return self.l_tree.predict_row(row)\n",
    "        else:\n",
    "            return self.r_tree.predict_row(row)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.array([]).reshape([0, 13])\n",
    "# train_label = np.array([])\n",
    "# test_data = []\n",
    "\n",
    "# for i in range(1, 6):\n",
    "#     train_data = np.concatenate((train_data, pd.read_csv('../input/train%d.csv' % i, header=None).values))\n",
    "#     train_label = np.concatenate((train_label, pd.read_csv('../input/label%d.csv' % i, header=None).values.squeeze(-1)))\n",
    "# for i in range(1, 7):\n",
    "#     test_data.append(np.array(pd.read_csv('../input/test%d.csv' % i, header=None).values))\n",
    "    \n",
    "# print(train_data.shape)\n",
    "# print(test_data[0].shape * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit with 4 workers\n",
      "fit with 4 workers\n",
      "fit with 4 workers\n",
      "fit with 4 workers\n",
      "fit with 4 workers\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_trees=16, sample_size=100000, min_leaf_size=300, max_depth=25)\n",
    "for i in range(1, 6):\n",
    "    train_data = pd.read_csv('../input/train%d.csv' % i, header=None).values\n",
    "    train_label = pd.read_csv('../input/label%d.csv' % i, header=None).values.squeeze(-1)\n",
    "    model.fit(train_data, train_label)\n",
    "    train_data = None\n",
    "    train_label = None\n",
    "    gc.collect()\n",
    "# for i in range(1, 6):\n",
    "#     for j in range(1, 6):\n",
    "#         if i == j:\n",
    "#             continue\n",
    "#         train_data = pd.read_csv('../input/train%d.csv' % i, header=None).values\n",
    "#         train_data = np.concatenate((train_data, pd.read_csv('../input/train%d.csv' % j, header=None).values))\n",
    "#         train_label = pd.read_csv('../input/label%d.csv' % i, header=None).values.squeeze(-1)\n",
    "#         train_label = np.concatenate((train_label, pd.read_csv('../input/label%d.csv' % j, header=None).values.squeeze(-1)))\n",
    "#         model.fit(train_data, train_label)\n",
    "#         train_data = None\n",
    "#         train_label = None\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict with 4 workers\n",
      "predict with 4 workers\n",
      "predict with 4 workers\n",
      "predict with 4 workers\n",
      "predict with 4 workers\n",
      "predict with 4 workers\n",
      "(10915121,)\n",
      "[-0.15689916 -0.1507478  -0.20327251 ...  0.02434633 -0.03409441\n",
      "  0.145811  ]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([])\n",
    "for i in range(1, 7):\n",
    "    test_data = np.array(pd.read_csv('../input/test%d.csv' % i, header=None).values)\n",
    "    arr = np.concatenate((arr, model.predict(test_data)))\n",
    "    test_data = None\n",
    "    gc.collect()\n",
    "print(arr.shape)\n",
    "print(arr)\n",
    "df = pd.DataFrame({ 'id': range(1, arr.size + 1), 'Predicted': arr })\n",
    "df.to_csv('./rfr.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
