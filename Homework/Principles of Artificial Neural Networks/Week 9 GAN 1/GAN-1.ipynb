{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week9: GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验要求与基本流程\n",
    "\n",
    "### 实验要求\n",
    "1. 结合理论课内容，深入理解GAN(Generative Adversarial Networks,生成对抗网络)的原理与训练过程.了解GAN网络结构的演变过程与几个基本的GAN的原理(如DCGAN,wGAN等.)\n",
    "2. 阅读实验指导书的实验内容,按照提示运行以及补充实验代码,或者简要回答问题.提交作业时,保留实验结果.\n",
    "\n",
    "\n",
    "### 实验流程\n",
    "* GAN的网络结构与训练\n",
    "* DCGAN\n",
    "* LSGAN\n",
    "* WGAN\n",
    "* WGAN-GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN(Generative Adversarial Networks)\n",
    "\n",
    "让我们先来看一个只是用线性层的生成对抗网络(GAN),来简单了解一下GAN的基本网络结构与训练过程.\n",
    "\n",
    "这个GAN网络结构分为两部分,生成器网络Generator和判别器网络Discriminator.\n",
    "- 生成器Generator将随机生成的噪声z通过多个线性层生成图片,注意生成器的最后一层是Tanh,所以我们生成的图片的取值范围为\\[-1,1\\],同理,我们会将真实图片归一化(normalize)到\\[-1,1\\].\n",
    "- 而判别器Discriminator是一个二分类器,通过多个线性层得到一个概率值来判别图片是\"真实\"或者是\"生成\"的,所以在Discriminator的最后是一个sigmoid,来得到图片是\"真实\"的概率.\n",
    "\n",
    "在所有的网络结构中我们都使用了LeakyReLU作为激活函数,除了G与D的最后一层,同时,我们在层与层之间我们还加入了BatchNormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, image_size=32, latent_dim=100, output_channel=1):\n",
    "        \"\"\"\n",
    "        image_size: image with and height\n",
    "        latent dim: the dimension of random noise z\n",
    "        output_channel: the channel of generated image, for example, 1 for gray image, 3 for RGB image\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_channel = output_channel\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Linear layer: latent_dim -> 128 -> 256 -> 512 -> 1024 -> output_channel * image_size * image_size -> Tanh\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Linear(1024, output_channel * image_size * image_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), self.output_channel, self.image_size, self.image_size)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size=32, input_channel=1):\n",
    "        \"\"\"\n",
    "        image_size: image with and height\n",
    "        input_channel: the channel of input image, for example, 1 for gray image, 3 for RGB image\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.input_channel = input_channel\n",
    "        \n",
    "        # Linear layer: input_channel * image_size * image_size -> 1024 -> 512 -> 256 -> 1 -> Sigmoid\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_channel * image_size * image_size, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        out = self.model(img_flat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "在训练我们的GAN网络之前, 先介绍一下本次实验可训练GAN的数据集,我们提供了两个数据集来供大家进行尝试数据集.\n",
    "- MNIST手写体3类数据集,这里为了加快我们的训练速度,我们提供了一个简化版本的只包含数字0,2的2类MNIST数据集,每类各1000张.图片为28\\*28的单通道灰度图(我们将其resize到32\\*32),对于GAN而言,我们不需要测试集.我们本次实验主要使用该数据集作为主要的训练数据集.\n",
    "\n",
    "\n",
    "- 室内家具数据集.为了加快我们的训练速度,我们将其做了删减处理,仅包含chair等一个类,共500张.图片为32\\*32的3通道彩色图片.\n",
    "\n",
    "\n",
    "\n",
    "下面是两个加载数据集的函数.注意我们将所有图片normalize到了\\[-1,1\\]之间.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"\n",
    "    load mnist(0,1,2) dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        # transform to 1-channel gray image since we reading image in RGB mode\n",
    "        transforms.Grayscale(1),\n",
    "        # resize image from 28 * 28 to 32 * 32\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        # normalize with mean=0.5 std=0.5\n",
    "        transforms.Normalize(mean=(0.5, ), \n",
    "                             std=(0.5, ))\n",
    "        ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(root='./data/mnist', transform=transform)\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "def load_furniture_data():\n",
    "    \"\"\"\n",
    "    load furniture dataset \n",
    "    \"\"\"\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # normalize with mean=0.5 std=0.5\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                             std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root='./data/household_furniture', transform=transform)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*无需阅读理解*)运行下面2个cell的代码来查看两个数据集中的20张随机真实图片."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    # denormalize\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show\n",
    "\"\"\"\n",
    "you can pass code in this cell\n",
    "\"\"\"\n",
    "# show mnist real data\n",
    "train_dataset = load_mnist_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "show(torchvision.utils.make_grid(denorm(next(iter(trainloader))[0]), nrow=5))\n",
    "# show furniture real data\n",
    "train_dataset = load_furniture_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "show(torchvision.utils.make_grid(denorm(next(iter(trainloader))[0]), nrow=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码实现GAN在一个epoch内的训练过程.\n",
    "\n",
    "大体而言,GAN的训练过程分为两步,首先将随机噪声z喂给G,生成图片,然后将真实图片和G生成的图片喂给D,然后使用对应的loss函数反向传播优化D.然后再次使用G生成图片,并喂给D,并使用对应的loss函数反向传播优化G.\n",
    "\n",
    "下面的图片是普通的GAN在G和D上的优化目标:\n",
    "![gan_d](./pictures/gan_d.png)\n",
    "![gan_g](./pictures/gan_g.png)\n",
    "值得注意的是,上述图片描述的是G和D的**优化目标**,而在具体实现过程中,我们实现loss函数来达到优化目标.对于上图中D与G的优化目标我们可以使用Binary Cross Entroy损失函数来实现:\n",
    "$$\n",
    "BCEloss(p_i,y_i)= -(y_i\\log{p_i}+(1−y_i)\\log{(1−p_i)})\n",
    "$$\n",
    "$p_i$, $y_i$分别是模型的预测值与图片的真实标签(1为真,0为假).因此,对于D,最大化其优化目标可以通过最小化一个BCEloss来实现,其真实图片$x\\sim{P_r}$的标签设置为1,而生成图片$z\\sim{P(z)}$的标签设置为0.我们可以看到这样的损失函数相当于对D的优化目标加上负号.\n",
    "\n",
    "而对于G,也通过最小化一个BCEloss来实现,即将生成图片$z\\sim{P(z)}$的标签设置为1即可,我们可以看到这样的损失函数与其优化目标是一致的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, G, D, G_optimizer, D_optimizer, loss_func, device, z_dim):\n",
    "    \"\"\"\n",
    "    train a GAN with model G and D in one epoch\n",
    "    Args:\n",
    "        trainloader: data loader to train\n",
    "        G: model Generator\n",
    "        D: model Discriminator\n",
    "        G_optimizer: optimizer of G(etc. Adam, SGD)\n",
    "        D_optimizer: optimizer of D(etc. Adam, SGD)\n",
    "        loss_func: loss function to train G and D. For example, Binary Cross Entropy(BCE) loss function\n",
    "        device: cpu or cuda device\n",
    "        z_dim: the dimension of random noise z\n",
    "    \"\"\"\n",
    "    # set train mode\n",
    "    D.train()\n",
    "    G.train()\n",
    "    \n",
    "    D_total_loss = 0\n",
    "    G_total_loss = 0\n",
    "    \n",
    "    \n",
    "    for i, (x, _) in enumerate(trainloader):\n",
    "        # real label and fake label\n",
    "        y_real = torch.ones(x.size(0), 1).to(device)\n",
    "        y_fake = torch.zeros(x.size(0), 1).to(device)\n",
    "        \n",
    "        x = x.to(device)\n",
    "        z = torch.rand(x.size(0), z_dim).to(device)\n",
    "\n",
    "        # update D network\n",
    "        # D optimizer zero grads\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # D real loss from real images\n",
    "        d_real = D(x)\n",
    "        d_real_loss = loss_func(d_real, y_real)\n",
    "        \n",
    "        # D fake loss from fake images generated by G\n",
    "        g_z = G(z)\n",
    "        d_fake = D(g_z)\n",
    "        d_fake_loss = loss_func(d_fake, y_fake)\n",
    "        \n",
    "        # D backward and step\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # update G network\n",
    "        # G optimizer zero grads\n",
    "        G_optimizer.zero_grad()\n",
    "        \n",
    "        # G loss\n",
    "        g_z = G(z)\n",
    "        d_fake = D(g_z)\n",
    "        g_loss = loss_func(d_fake, y_real)\n",
    "        \n",
    "        # G backward and step\n",
    "        g_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        D_total_loss += d_loss.item()\n",
    "        G_total_loss += g_loss.item()\n",
    "    \n",
    "    return D_total_loss / len(trainloader), G_total_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型训练后,我们需要查看此时G生成的图片效果,下面的visualize_results代码便实现了这块内容.注意,我们生成的图片都在\\[-1,1\\],因此,我们需要将图片反向归一化(denorm)到\\[0,1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(G, device, z_dim, result_size=20):\n",
    "    G.eval()\n",
    "    \n",
    "    z = torch.rand(result_size, z_dim).to(device)\n",
    "    g_z = G(z)\n",
    "    \n",
    "    show(torchvision.utils.make_grid(denorm(g_z.detach().cpu()), nrow=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "万事具备,接下来让我们来尝试这训练一个基本的GAN网络吧.这里实现run_gan函数来调用train以及visualize_results来训练我们的GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gan(trainloader, G, D, G_optimizer, D_optimizer, loss_func, n_epochs, device, latent_dim):\n",
    "    d_loss_hist = []\n",
    "    g_loss_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        d_loss, g_loss = train(trainloader, G, D, G_optimizer, D_optimizer, loss_func, device, \n",
    "                               z_dim=latent_dim)\n",
    "        print('Epoch {}: Train D loss: {:.4f}, G loss: {:.4f}'.format(epoch, d_loss, g_loss))\n",
    "\n",
    "        d_loss_hist.append(d_loss)\n",
    "        g_loss_hist.append(g_loss)\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "            visualize_results(G, device, latent_dim) \n",
    "    \n",
    "    return d_loss_hist, g_loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置好超参数就可以开始训练!让我们尝试用它来训练2类的mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "# z dim\n",
    "latent_dim = 100\n",
    "\n",
    "# image size and channel\n",
    "image_size=32\n",
    "image_channel=1\n",
    "\n",
    "# Adam lr and betas\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# epochs and batch size\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# mnist dataset and dataloader\n",
    "train_dataset = load_mnist_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# use BCELoss as loss function\n",
    "bceloss = nn.BCELoss().to(device)\n",
    "\n",
    "# G and D model\n",
    "G = Generator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = Discriminator(image_size=image_size, input_channel=image_channel).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, bceloss, \n",
    "                                   n_epochs, device, latent_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完后,让我们来看一下G生成的图片效果,可以看到即使是一个简单的GAN在这种简单的数据集上的生成效果还是不错的,虽然仍然存在不少瑕疵,比如说我们可以看到生成的图片上的数字有很多奇怪的雪花等等.\n",
    "\n",
    "让我们看一下G和D的loss变化曲线(运行下方语句.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **作业**:\n",
    "观察G与D的loss曲线,与之前的训练的CNN的loss曲线相比,有什么不同?试简要回答你觉得可能产生这样的不同的原因."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答:\n",
    "\n",
    "CNN的loss曲线通常是一开始下降得很快，等到迭代多次之后，loss曲线下降的速度将变慢，并且可能产生振荡，loss值可能上升。这是因为CNN训练一开始距离极值还很远，训练效率高。等到多次迭代之后，CNN可能已经接近拟合，甚至可能出现过拟合，导致loss值振荡。\n",
    "\n",
    "而生成网络和判别网络的loss曲线与CNN的曲线都有明显区别，生成网络的loss值是逐渐变大，判别网络的loss值是逐渐变小。一开始，生成网络的loss值急剧上升，因为一开始生成网络受真实图片的影响小。随着迭代次数的增多，判别网络判断真假图的能力增大，生成网络的loss值迅速增大，而判别网络的loss值下降。\n",
    "\n",
    "迭代多次之后，生成网络和判别网络开始对抗，生成网络的loss值可能因此有下降的概率，而判别网络也因此有上升的概率。因为生成网络生成假图以假乱真的能力增大，判别网络判别真假图的能力也增大，两者开始对抗。\n",
    "\n",
    "在长期的若干次迭代中，生成网络的loss值趋向于增大，而判别网络趋向于下降。因为数据集的数量是有限的，网络最终必然倾向于拟合。loss曲线中判别器下降而生成器上升，是因为判别网络的判别能力增大的速度比生成网络的能力要快，也可能是生成网络已经达到拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在DCGAN(Deep Convolution GAN)中,最大的改变是使用了CNN代替全连接层.在生成器G中,使用stride为2的转置卷积来生成图片同时扩大图片尺寸,而在判别器D中,使用stride为2的卷积来将图片进行卷积并下采样.除此之外,DCGAN加入了在层与层之间BatchNormalization(虽然我们在普通的GAN中就已经添加),在G中使用ReLU作为激活函数,而在D中使用LeakyReLU作为激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import initialize_weights\n",
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self, image_size=32, latent_dim=64, output_channel=1):\n",
    "        super(DCGenerator, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_channel = output_channel\n",
    "        \n",
    "        self.init_size = image_size // 8\n",
    "        \n",
    "        # fc: Linear -> BN -> ReLU\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512 * self.init_size ** 2),\n",
    "            nn.BatchNorm1d(512 * self.init_size ** 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # deconv: ConvTranspose2d(4, 2, 1) -> BN -> ReLU -> \n",
    "        #         ConvTranspose2d(4, 2, 1) -> BN -> ReLU -> \n",
    "        #         ConvTranspose2d(4, 2, 1) -> Tanh\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, output_channel, 4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.shape[0], 512, self.init_size, self.init_size)\n",
    "        img = self.deconv(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self, image_size=32, input_channel=1, sigmoid=True):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.input_channel = input_channel\n",
    "        self.fc_size = image_size // 8\n",
    "        \n",
    "        # conv: Conv2d(3,2,1) -> LeakyReLU \n",
    "        #       Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "        #       Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # fc: Linear -> Sigmoid\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * self.fc_size * self.fc_size, 1),\n",
    "        )\n",
    "        if sigmoid:\n",
    "            self.fc.add_module('sigmoid', nn.Sigmoid())\n",
    "        initialize_weights(self)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的,我们使用同样的mnist数据集对DCGAN进行训练."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "# z dim\n",
    "latent_dim = 100\n",
    "\n",
    "# image size and channel\n",
    "image_size=32\n",
    "image_channel=1\n",
    "\n",
    "# Adam lr and betas\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# epochs and batch size\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# mnist dataset and dataloader\n",
    "train_dataset = load_mnist_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# use BCELoss as loss function\n",
    "bceloss = nn.BCELoss().to(device)\n",
    "\n",
    "# G and D model, use DCGAN\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, bceloss, \n",
    "                                   n_epochs, device, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到,DCGAN的生成图片质量比起只有线性层的GAN要好不少.接下来,让我们尝试使用家具数据集来训练DCGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RGB image channel = 3\n",
    "image_channel=3\n",
    "\n",
    "# epochs\n",
    "n_epochs = 300\n",
    "batch_size = 32\n",
    "image_size=32\n",
    "latent_dim = 100\n",
    "\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "bceloss = nn.BCELoss().to(device)\n",
    "\n",
    "# mnist dataset and dataloader\n",
    "train_dataset = load_furniture_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# G and D model, use DCGAN\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, bceloss, \n",
    "                                   n_epochs, device, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSGAN(Least Squares GAN)将loss函数改为了 L2损失.G和D的优化目标如下图所示,\n",
    "![lsgan](pictures/lsgan.png)\n",
    "#### **作业**: \n",
    "在这里,请在下方补充L2Loss的代码来实现L2损失来优化上面的目标.并使用这个loss函数在mnist数据集上训练LSGAN,并显示训练的效果图片及loss变化曲线.\n",
    "\n",
    "提示:忽略上图的1/2.L2损失即MSEloss(均方误差),传入两个参数input_是指判别器D预测为\"真实\"的概率值(size为batch_size\\*1),target为标签1或0(size为batch_size\\*1).只允许使用pytorch和python的运算实现(不能直接调用MSEloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L2Loss, self).__init__()\n",
    "    \n",
    "    def forward(self, input_, target):\n",
    "        \"\"\"\n",
    "        input_: (batch_size*1) \n",
    "        target: (batch_size*1) labels, 1 or 0\n",
    "        \"\"\"\n",
    "        return ((input_ - target) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成上方代码后,使用所写的L2Loss在mnist数据集上训练DCGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "# z dim\n",
    "latent_dim = 100\n",
    "\n",
    "# image size and channel\n",
    "image_size=32\n",
    "image_channel=1\n",
    "\n",
    "# Adam lr and betas\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# epochs and batch size\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# mnist dataset and dataloader\n",
    "train_dataset = load_mnist_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# use L2Loss as loss function\n",
    "l2loss = L2Loss().to(device)\n",
    "\n",
    "# G and D model, use DCGAN\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, l2loss, n_epochs, device, \n",
    "                                   latent_dim)\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN依然存在着训练不稳定,模式崩溃(collapse mode,可以理解为生成的图片多样性极低)的问题(我们的数据集不一定能体现出来).WGAN(Wasserstein GAN)将传统GAN中拟合的JS散度改为Wasserstein距离.WGAN一定程度上解决了GAN训练不稳定以及模式奔溃的问题.\n",
    "\n",
    "WGAN的判别器的优化目标变为,在满足Lipschitz连续的条件(我们可以限制w不超过某个范围来满足)下,最大化\n",
    "![1](pictures/wgan1.png)\n",
    "而它会近似于真实分布与生成分布之间的Wasserstein距离.所以我们D和G的loss函数变为:\n",
    "![1](pictures/wgan3.png) ![1](pictures/wgan2.png)\n",
    "\n",
    "\n",
    "具体到在实现上,WGAN主要有3点改变:\n",
    "- 判别器D最后一层去掉sigmoid\n",
    "- 生成器G和判别器的loss不使用log\n",
    "- 每次更新判别器D后,将参数的绝对值截断到某一个固定常数c\n",
    "\n",
    "所以我们主要重写了WGAN的训练函数,在这里,网络结构使用*去除Sigmoid的DCGAN*(注意初始化D时将sigmoid设置为False来去掉最后一层sigmoid).\n",
    "\n",
    "下面是WGAN的代码实现.加入了两个参数,n_d表示每训练一次G训练D的次数,weight_clip表示截断的常数.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgan_train(trainloader, G, D, G_optimizer, D_optimizer, device, z_dim, n_d=2, weight_clip=0.01):\n",
    "    \n",
    "    \"\"\"\n",
    "    n_d: the number of iterations of D update per G update iteration\n",
    "    weight_clip: the clipping parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    D.train()\n",
    "    G.train()\n",
    "    \n",
    "    D_total_loss = 0\n",
    "    G_total_loss = 0\n",
    "    \n",
    "    for i, (x, _) in enumerate(trainloader):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        \n",
    "        # update D network\n",
    "        # D optimizer zero grads\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # D real loss from real images\n",
    "        d_real = D(x)\n",
    "        d_real_loss = - d_real.mean()\n",
    "        \n",
    "        # D fake loss from fake images generated by G\n",
    "        z = torch.rand(x.size(0), z_dim).to(device)\n",
    "        g_z = G(z)\n",
    "        d_fake = D(g_z)\n",
    "        d_fake_loss = d_fake.mean()\n",
    "        \n",
    "        # D backward and step\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # D weight clip\n",
    "        for params in D.parameters():\n",
    "            params.data.clamp_(-weight_clip, weight_clip)\n",
    "            \n",
    "        D_total_loss += d_loss.item()\n",
    "\n",
    "        # update G network\n",
    "        if (i + 1) % n_d == 0:\n",
    "            # G optimizer zero grads\n",
    "            G_optimizer.zero_grad()\n",
    "\n",
    "            # G loss\n",
    "            g_z = G(z)\n",
    "            d_fake = D(g_z)\n",
    "            g_loss = - d_fake.mean()\n",
    "\n",
    "            # G backward and step\n",
    "            g_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            \n",
    "            G_total_loss += g_loss.item()\n",
    "    \n",
    "    return D_total_loss / len(trainloader), G_total_loss * n_d / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wgan(trainloader, G, D, G_optimizer, D_optimizer, n_epochs, device, latent_dim, n_d, weight_clip):\n",
    "    d_loss_hist = []\n",
    "    g_loss_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        d_loss, g_loss = wgan_train(trainloader, G, D, G_optimizer, D_optimizer, device, \n",
    "                               z_dim=latent_dim, n_d=n_d, weight_clip=weight_clip)\n",
    "        print('Epoch {}: Train D loss: {:.4f}, G loss: {:.4f}'.format(epoch, d_loss, g_loss))\n",
    "\n",
    "        d_loss_hist.append(d_loss)\n",
    "        g_loss_hist.append(g_loss)\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "            visualize_results(G, device, latent_dim) \n",
    "    \n",
    "    return d_loss_hist, g_loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来让我们使用写好的run_wgan来跑我们的家具(椅子)数据集,看看效果如何."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "# z dim\n",
    "latent_dim = 100\n",
    "\n",
    "# image size and channel\n",
    "image_size=32\n",
    "image_channel=3\n",
    "\n",
    "# Adam lr and betas\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# epochs and batch size\n",
    "n_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "# n_d: the number of iterations of D update per G update iteration\n",
    "n_d = 2\n",
    "weight_clip=0.01\n",
    "\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# mnist dataset and dataloader\n",
    "train_dataset = load_furniture_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# G and D model, use DCGAN, note that sigmoid is removed in D\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel, sigmoid=False).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_wgan(trainloader, G, D, G_optimizer, D_optimizer, n_epochs, device, \n",
    "                                    latent_dim, n_d, weight_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由**WGAN**的原理我们知道,D_loss的*相反数*可以表示生成数据分布与真实分布的Wasserstein距离,其数值越小,表明两个分布越相似,GAN训练得越好.它的值给我们训练GAN提供了一个指标.\n",
    "\n",
    "运行下方代码观察wgan的loss曲线,可以看到,总体上,D_loss的*相反数*随着epoch数增加逐渐下降,同时生成的数据也越来越逼近真实数据,这与wgan的原理是相符合的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来运行下面两个cell的代码,集中展示wgan的参数分布."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_weights_hist\n",
    "def show_d_params(D):\n",
    "    plist = []\n",
    "    for params in D.parameters():\n",
    "        plist.extend(params.cpu().data.view(-1).numpy())\n",
    "    show_weights_hist(plist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_d_params(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到,参数都被截断在\\[-c, c\\]之间,大部分参数集中在-c和c附近."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **作业**: \n",
    "尝试使用n_d设置为5, 3, 1等,再次训练wGAN,n_d为多少时的结果最好?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "`n_d`为1时结果最好，虽然说`n_d`代表的是每训练一次G就训练多少次D，但在网络中是先训练D的，也就是每`n_d`批数据，才训练一次G。`n_d`为1时，G的训练次数是最多的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d = 1\n",
    "\n",
    "# G and D model, use DCGAN, note that sigmoid is removed in D\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel, sigmoid=False).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_wgan(trainloader, G, D, G_optimizer, D_optimizer, n_epochs, device, \n",
    "                                    latent_dim, n_d, weight_clip)\n",
    "\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d = 3\n",
    "\n",
    "# G and D model, use DCGAN, note that sigmoid is removed in D\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel, sigmoid=False).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_wgan(trainloader, G, D, G_optimizer, D_optimizer, n_epochs, device, \n",
    "                                    latent_dim, n_d, weight_clip)\n",
    "\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d = 5\n",
    "\n",
    "# G and D model, use DCGAN, note that sigmoid is removed in D\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel, sigmoid=False).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_wgan(trainloader, G, D, G_optimizer, D_optimizer, n_epochs, device, \n",
    "                                    latent_dim, n_d, weight_clip)\n",
    "\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP(improved wgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在WGAN中，需要进行截断, 在实验中发现： 对于比较深的WAGN，它不容易收敛。\n",
    "\n",
    "大致原因如下：\n",
    "1. 实验发现最后大多数的权重都在-c 和c上,这就意味了大部分权重只有两个可能数,这太简单了,作为一个深度神经网络来说,这实在是对它强大的拟合能力的浪费.\n",
    "2. 实验发现容易导致梯度消失或梯度爆炸。判别器是一个多层网络，如果把clip的值设得稍微小了一点,每经过一层网络,梯度就变小一点点,多层之后就会指数衰减；反之,则容易导致梯度爆炸.\n",
    "\n",
    "所以WGAN-GP使用了Gradient penalty(梯度惩罚）来代替clip.\n",
    "因为Lipschitz限制是要求判别器的梯度不超过K,所以可以直接使用一个loss term来实现这一点,所以改进后D的优化目标改进为如下:\n",
    "![wgan_gp](pictures/wgan-gp.svg)\n",
    "\n",
    "下面是WGAN-GP的具体代码实现,同WGAN,我们也只实现了他的训练代码,而模型我们直接使用DCGAN的模型."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "def wgan_gp_train(trainloader, G, D, G_optimizer, D_optimizer, device, z_dim, lambda_=10, n_d=2):\n",
    "    \n",
    "    D.train()\n",
    "    G.train()\n",
    "    \n",
    "    D_total_loss = 0\n",
    "    G_total_loss = 0\n",
    "    \n",
    "    \n",
    "    for i, (x, _) in enumerate(trainloader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        # update D network\n",
    "        # D optimizer zero grads\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # D real loss from real images\n",
    "        d_real = D(x)\n",
    "        d_real_loss = - d_real.mean()\n",
    "        \n",
    "        # D fake loss from fake images generated by G\n",
    "        z = torch.rand(x.size(0), z_dim).to(device)\n",
    "        g_z = G(z)\n",
    "        d_fake = D(g_z)\n",
    "        d_fake_loss = d_fake.mean()\n",
    "        \n",
    "        # D gradient penalty\n",
    "        \n",
    "        #   a random number epsilon\n",
    "        epsilon = torch.rand(x.size(0), 1, 1, 1).cuda()\n",
    "        x_hat = epsilon * x + (1 - epsilon) * g_z\n",
    "        x_hat.requires_grad_(True)\n",
    "\n",
    "        y_hat = D(x_hat)\n",
    "        #   computes the sum of gradients of y_hat with regard to x_hat\n",
    "        gradients = autograd.grad(outputs=y_hat, inputs=x_hat, grad_outputs=torch.ones(y_hat.size()).cuda(),\n",
    "                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        #   computes gradientpenalty\n",
    "        gradient_penalty =  torch.mean((gradients.view(gradients.size()[0], -1).norm(p=2, dim=1) - 1) ** 2)\n",
    "        \n",
    "        # D backward and step\n",
    "        d_loss = d_real_loss + d_fake_loss + lambda_ * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        \n",
    "            \n",
    "        D_total_loss += d_loss.item()\n",
    "\n",
    "        # update G network\n",
    "        # G optimizer zero grads\n",
    "        if (i + 1) % n_d == 0:\n",
    "            G_optimizer.zero_grad()\n",
    "\n",
    "            # G loss\n",
    "            g_z = G(z)\n",
    "            d_fake = D(g_z)\n",
    "            g_loss = - d_fake.mean()\n",
    "\n",
    "            # G backward and step\n",
    "            g_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            \n",
    "            G_total_loss += g_loss.item()\n",
    "    \n",
    "    return D_total_loss / len(trainloader), G_total_loss * n_d / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "# z dim\n",
    "latent_dim = 100\n",
    "\n",
    "# image size and channel\n",
    "image_size=32\n",
    "image_channel=3\n",
    "\n",
    "# Adam lr and betas\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# epochs and batch size\n",
    "n_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# n_d: train D\n",
    "n_d = 2\n",
    "lambda_ = 10\n",
    "\n",
    "# mnist dataset and dataloader\n",
    "train_dataset = load_furniture_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# G and D model, use DCGAN, note that sigmoid is removed in D\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel).to(device)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel, sigmoid=False).to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist = []\n",
    "g_loss_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    d_loss, g_loss = wgan_gp_train(trainloader, G, D, G_optimizer, D_optimizer, device, \n",
    "                           z_dim=latent_dim, lambda_=lambda_, n_d=n_d)\n",
    "    print('Epoch {}: Train D loss: {:.4f}, G loss: {:.4f}'.format(epoch, d_loss, g_loss))\n",
    "    \n",
    "    d_loss_hist.append(d_loss)\n",
    "    g_loss_hist.append(g_loss)\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        visualize_results(G, device, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理,观察loss曲线和D上的参数分布."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_d_params(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **作业**: \n",
    "观察WGAN和WGAN-GP生成器生成的图片效果,它们在相同epoch时生成的图片效果(或者说生成图片达到效果所需要epoch数量),它们的loss曲线以及D的参数分布,说说有什么不同?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "可以观察到，在相同epoch时WGAN-GP生成的图片效果更加好。相比WGAN，WGAN-GP的收敛速度更加快。在训练的一开始可以看到WGAN-GP很快就能生成了图片物体的大致轮廓，而WGAN相对慢一点。\n",
    "\n",
    "WGAN的loss曲线中，D的loss曲线的相反数和G的loss曲线是逐渐下降的。而WGAN-GP的loss曲线中，G的loss值是逐渐上升的。\n",
    "\n",
    "WGAN中的D的参数主要分布在-c和c两处，而WGAN-GP的D的参数主要分布在0左右。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
