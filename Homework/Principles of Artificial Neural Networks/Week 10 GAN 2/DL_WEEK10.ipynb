{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week10: GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验要求与基本流程\n",
    "\n",
    "### 实验要求\n",
    "1. 完成上一节实验课内容，理解GAN(Generative Adversarial Networks,生成对抗网络)的原理与训练方法.\n",
    "2. 结合理论课内容, 了解CGAN, pix2pix等模型基本结构和主要作用.\n",
    "2. 阅读实验指导书的实验内容,按照提示运行以及补充实验代码,或者简要回答问题.提交作业时,保留实验结果.\n",
    "\n",
    "\n",
    "### 实验流程\n",
    "* CGAN\n",
    "* pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGAN(Conditional GAN)\n",
    "由上节课的内容可以看到,GAN可以用来生成接近真实的图片,但普通的GAN太过自由而不可控了,而CGAN(Conditional GAN)是一种带条件约束的GAN，在生成模型(D)和判别模型(G)的建模中均引入条件变量.这些条件变量可以基于多种信息，例如类别标签，用于图像修复的部分数据等等.在这个接下来这个CGAN中我们引入类别标签作为G和D的条件变量.\n",
    "\n",
    "在下面的CGAN网络结构(与上节课展示的DCGAN模型相似)中,与之前的模型最大的不同是在G和D的输入中加入了类别标签labels,在G中,labels(用one-hot向量表示,如有3个类(0/1/2),第2类的one-hot向量为\\[0, 0, 1\\])和原来的噪声z一起输入到第一层全连接层中,在D中,labels和输入图片一起输入到卷积层中,labels中每个label用大小为(class_num,image_size,image_size)的张量表示,其正确类别的channel全为1,其余channel全为0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import initialize_weights\n",
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self, image_size=32, latent_dim=64, output_channel=1, class_num=3):\n",
    "        super(DCGenerator, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_channel = output_channel\n",
    "        self.class_num = class_num\n",
    "        \n",
    "        self.init_size = image_size // 8\n",
    "        \n",
    "        # fc: Linear -> BN -> ReLU\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + class_num, 512 * self.init_size ** 2),\n",
    "            nn.BatchNorm1d(512 * self.init_size ** 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "            \n",
    "        # deconv: ConvTranspose2d(4, 2, 1) -> BN -> ReLU -> \n",
    "        #         ConvTranspose2d(4, 2, 1) -> BN -> ReLU -> \n",
    "        #         ConvTranspose2d(4, 2, 1) -> Tanh\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, output_channel, 4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        \"\"\"\n",
    "        z : noise vector\n",
    "        labels : one-hot vector \n",
    "        \"\"\"\n",
    "        input_ = torch.cat((z, labels), dim=1)\n",
    "        out = self.fc(input_)\n",
    "        out = out.view(out.shape[0], 512, self.init_size, self.init_size)\n",
    "        img = self.deconv(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self, image_size=32, input_channel=1, class_num=3, sigmoid=True):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.input_channel = input_channel\n",
    "        self.class_num = class_num\n",
    "        \n",
    "        self.fc_size = image_size // 8\n",
    "        \n",
    "        # conv: Conv2d(3,2,1) -> LeakyReLU \n",
    "        #       Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "        #       Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "        \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channel + class_num, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # fc: Linear -> Sigmoid\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * self.fc_size * self.fc_size, 1),\n",
    "        )\n",
    "        if sigmoid:\n",
    "            self.fc.add_module('sigmoid', nn.Sigmoid())\n",
    "        initialize_weights(self)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        \"\"\"\n",
    "        img : input image\n",
    "        labels : (batch_size, class_num, image_size, image_size)\n",
    "                the i-th channel is filled with 1, and others is filled with 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_ = torch.cat((img, labels), dim=1)\n",
    "        out = self.conv(input_)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "我们使用我们熟悉的MNIST手写体数据集来训练我们的CGAN,我们同样提供了一个简化版本的数据集来加快我们的训练速度,与上次的数据集不一样的是,这次的数据集包含0到9共10类的手写数字,每类各200张,共2000张.图片同样为28\\*28的单通道灰度图(我们将其resize到32\\*32).下面是加载mnist数据集的代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"\n",
    "    load mnist(0,1,2) dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        # transform to 1-channel gray image since we reading image in RGB mode\n",
    "        transforms.Grayscale(1),\n",
    "        # resize image from 28 * 28 to 32 * 32\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        # normalize with mean=0.5 std=0.5\n",
    "        transforms.Normalize(mean=(0.5, ), \n",
    "                             std=(0.5, ))\n",
    "        ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(root='./data/mnist', transform=transform)\n",
    "    \n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来让我们查看一下各个类上真实的手写体数据集的数据吧.(运行一下2个cell的代码,无需理解)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    # denormalize\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show\n",
    "\"\"\"\n",
    "you can pass code in this cell\n",
    "\"\"\"\n",
    "# show mnist real data\n",
    "train_dataset = load_mnist_data()\n",
    "images = []\n",
    "for j in range(5):\n",
    "    for i in range(10):\n",
    "        images.append(train_dataset[i * 200 + j][0])\n",
    "show(torchvision.utils.make_grid(denorm(torch.stack(images)), nrow=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练部分的代码代码与之前相似, 不同的地方在于要根据类别生成y_vec(one-hot向量如类别2对应\\[0,1,0,0,0,0,0,0,0,0\\])和y_fill(将y_vec扩展到大小为(class_num, image_size, image_size),正确的类别的channel全为1,其他channel全为0),分别输入G和D作为条件变量.其他训练过程与普通的GAN相似.我们可以先为每个类别标签生成vecs和fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class number\n",
    "class_num = 10\n",
    "\n",
    "# image size and channel\n",
    "image_size=32\n",
    "image_channel=1\n",
    "\n",
    "# vecs: one-hot vectors of size(class_num, class_num)\n",
    "# fills: vecs expand to size(class_num, class_num, image_size, image_size)\n",
    "vecs = torch.eye(class_num)\n",
    "fills = vecs.unsqueeze(2).unsqueeze(3).expand(class_num, class_num, image_size, image_size)\n",
    "\n",
    "print(vecs)\n",
    "print(fills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, G, D, G_optimizer, D_optimizer, loss_func, device, z_dim, class_num):\n",
    "    \"\"\"\n",
    "    train a GAN with model G and D in one epoch\n",
    "    Args:\n",
    "        trainloader: data loader to train\n",
    "        G: model Generator\n",
    "        D: model Discriminator\n",
    "        G_optimizer: optimizer of G(etc. Adam, SGD)\n",
    "        D_optimizer: optimizer of D(etc. Adam, SGD)\n",
    "        loss_func: Binary Cross Entropy(BCE) or MSE loss function\n",
    "        device: cpu or cuda device\n",
    "        z_dim: the dimension of random noise z\n",
    "        \n",
    "    \"\"\"\n",
    "    # set train mode\n",
    "    D.train()\n",
    "    G.train()\n",
    "    \n",
    "    D_total_loss = 0\n",
    "    G_total_loss = 0\n",
    "    \n",
    "    \n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x = x.to(device)\n",
    "        batch_size_ = x.size(0)\n",
    "        image_size = x.size(2)\n",
    "        \n",
    "        # real label and fake label\n",
    "        real_label = torch.ones(batch_size_, 1).to(device)\n",
    "        fake_label = torch.zeros(batch_size_, 1).to(device)\n",
    "        \n",
    "        # y_vec: (batch_size, class_num) one-hot vector, for example, [0,0,0,0,1,0,0,0,0,0] (label: 4)\n",
    "        y_vec = vecs[y.long()].to(device)\n",
    "        # y_fill: (batch_size, class_num, image_size, image_size)\n",
    "        # y_fill: the i-th channel is filled with 1, and others is filled with 0.\n",
    "        y_fill = fills[y.long()].to(device)\n",
    "        \n",
    "        z = torch.rand(batch_size_, z_dim).to(device)\n",
    "\n",
    "        # update D network\n",
    "        # D optimizer zero grads\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        # D real loss from real images\n",
    "        d_real = D(x, y_fill)\n",
    "        d_real_loss = loss_func(d_real, real_label)\n",
    "        \n",
    "        # D fake loss from fake images generated by G\n",
    "        g_z = G(z, y_vec)\n",
    "        d_fake = D(g_z, y_fill)\n",
    "        d_fake_loss = loss_func(d_fake, fake_label)\n",
    "        \n",
    "        # D backward and step\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # update G network\n",
    "        # G optimizer zero gradsinput_dim=100, output_dim=1, input_size=32, class_num=10\n",
    "        G_optimizer.zero_grad()\n",
    "        \n",
    "        # G loss\n",
    "        g_z = G(z, y_vec)\n",
    "        d_fake = D(g_z, y_fill)\n",
    "        g_loss = loss_func(d_fake, real_label)\n",
    "        \n",
    "        # G backward and step\n",
    "        g_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        D_total_loss += d_loss.item()\n",
    "        G_total_loss += g_loss.item()\n",
    "        \n",
    "    \n",
    "    return D_total_loss / len(trainloader), G_total_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize_results和run_gan的代码不再详细说明."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(G, device, z_dim, class_num, class_result_size=5):\n",
    "    G.eval()\n",
    "    \n",
    "    z = torch.rand(class_num * class_result_size, z_dim).to(device)\n",
    "    \n",
    "    y = torch.LongTensor([i for i in range(class_num)] * class_result_size)\n",
    "    y_vec = vecs[y.long()].to(device)\n",
    "    g_z = G(z, y_vec)\n",
    "    \n",
    "    show(torchvision.utils.make_grid(denorm(g_z.detach().cpu()), nrow=class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gan(trainloader, G, D, G_optimizer, D_optimizer, loss_func, n_epochs, device, latent_dim, class_num):\n",
    "    d_loss_hist = []\n",
    "    g_loss_hist = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        d_loss, g_loss = train(trainloader, G, D, G_optimizer, D_optimizer, loss_func, device, \n",
    "                               latent_dim, class_num)\n",
    "        print('Epoch {}: Train D loss: {:.4f}, G loss: {:.4f}'.format(epoch, d_loss, g_loss))\n",
    "\n",
    "        d_loss_hist.append(d_loss)\n",
    "        g_loss_hist.append(g_loss)\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "            visualize_results(G, device, latent_dim, class_num) \n",
    "    \n",
    "    return d_loss_hist, g_loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面尝试训练一下我们的CGAN吧."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "# z dim\n",
    "latent_dim = 100\n",
    "\n",
    "# Adam lr and betas\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# epochs and batch size\n",
    "n_epochs = 120\n",
    "batch_size = 32\n",
    "\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# mnist dataset and dataloader\n",
    "train_dataset = load_mnist_data()\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# use BCELoss as loss function\n",
    "bceloss = nn.BCELoss().to(device)\n",
    "\n",
    "# G and D model\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel, class_num=class_num)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel, class_num=class_num)\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "print(D)\n",
    "print(G)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, bceloss, \n",
    "                                   n_epochs, device, latent_dim, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import loss_plot\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业** : \n",
    "1. 在D中,可以将输入图片和labels分别通过两个不同的卷积层然后在维度1合并(通道上合并),再一起送去接下来的网络结构.网络部分结构已经在DCDiscriminator中写好,请在**补充forward函数**完成上述功能并再次使用同样的数据集训练CGAN.与之前的结果对比,说说有什么不同?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDiscriminator1(nn.Module):\n",
    "    def __init__(self, image_size=32, input_channel=1, class_num=3, sigmoid=True):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.input_channel = input_channel\n",
    "        self.class_num = class_num\n",
    "        \n",
    "        self.fc_size = image_size // 8\n",
    "\n",
    "        #       model : img -> conv1_1\n",
    "        #               labels -> conv1_2    \n",
    "        #       (img U labels) -> Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "        #       Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "\n",
    "        self.conv1_1 = nn.Sequential(nn.Conv2d(input_channel, 64, 3, 2, 1),\n",
    "                                     nn.BatchNorm2d(64))\n",
    "        self.conv1_2 = nn.Sequential(nn.Conv2d(class_num, 64, 3, 2, 1),\n",
    "                                     nn.BatchNorm2d(64))\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # fc: Linear -> Sigmoid\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * self.fc_size * self.fc_size, 1),\n",
    "        )\n",
    "        if sigmoid:\n",
    "            self.fc.add_module('sigmoid', nn.Sigmoid())\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        \"\"\"\n",
    "        img : input image\n",
    "        labels : (batch_size, class_num, image_size, image_size)\n",
    "                the i-th channel is filled with 1, and others is filled with 0.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        To Do\n",
    "        \"\"\"\n",
    "        input_img = self.conv1_1(img)\n",
    "        input_labels = self.conv1_2(labels)\n",
    "        input_ = torch.cat((input_img, input_labels), dim=1)\n",
    "        out = self.conv(input_)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# G and D model\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel, class_num=class_num)\n",
    "D = DCDiscriminator1(image_size=image_size, input_channel=image_channel, class_num=class_num)\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, bceloss, \n",
    "                                   n_epochs, device, latent_dim, class_num)\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "观察两次训练的loss曲线，可以发现给图片和标签加上卷积之后，G的loss值一直稳定在一定的范围内，而没加上卷积处理的网络中，G的loss值一开始很低，后来逐渐升高。从loss曲线上分析，在第一次训练中G的变化更大。因此，第二次训练能得到效果更好的生成器。\n",
    "\n",
    "从输出的图片上比较，也可以很明显可以看到第二次训练输出的结果比第一次好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在D中,可以将输入图片通过1个卷积层然后和(尺寸与输入图片一致的)labels在维度1合并(通道上合并),再一起送去接下来的网络结构.网络部分结构已经在DCDiscriminator中写好,请在**补充forward函数**完成上述功能,并再次使用同样的数据集训练CGAN.与之前的结果对比,说说有什么不同?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDiscriminator2(nn.Module):\n",
    "    def __init__(self, image_size=32, input_channel=1, class_num=3, sigmoid=True):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.input_channel = input_channel\n",
    "        self.class_num = class_num\n",
    "        \n",
    "        self.fc_size = image_size // 8\n",
    "\n",
    "        #       model : img -> conv1\n",
    "        #               labels -> maxpool   \n",
    "        #       (img U labels) -> Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "        #       Conv2d(3,2,1) -> BN -> LeakyReLU \n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_channel, 128, 3, 2, 1),\n",
    "                                     nn.BatchNorm2d(128))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128 + class_num, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # fc: Linear -> Sigmoid\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * self.fc_size * self.fc_size, 1),\n",
    "        )\n",
    "        if sigmoid:\n",
    "            self.fc.add_module('sigmoid', nn.Sigmoid())\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        \"\"\"\n",
    "        img : input image\n",
    "        labels : (batch_size, class_num, image_size, image_size)\n",
    "                the i-th channel is filled with 1, and others is filled with 0.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        To Do\n",
    "        \"\"\"\n",
    "        input_img = self.conv1(img)\n",
    "        input_labels = self.maxpool(labels)\n",
    "        input_ = torch.cat((input_img, input_labels), dim=1)\n",
    "        out = self.conv(input_)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# G and D model\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel, class_num=class_num)\n",
    "D = DCDiscriminator2(image_size=image_size, input_channel=image_channel, class_num=class_num)\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, bceloss, \n",
    "                                   n_epochs, device, latent_dim, class_num)\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "可以观察到生成网络的loss曲线变化的程度要较前两次训练的小，说明得到的G的能力相比前两次训练得到的G要强。\n",
    "\n",
    "不过，在最终输出的图片中，肉眼分辨是效果比前两次训练差，这大概是输出选择的那一代中G的效果比较差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 若输入的类别标签不用one-hot的向量表示,我们一开始先为每个类随机生成一个随机向量,然后使用这个向量作为类别标签,这样对结果会有改变吗?试尝试运行下面代码,与之前的结果对比,说说有什么不同?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = torch.randn(class_num, class_num)\n",
    "fills = vecs.unsqueeze(2).unsqueeze(3).expand(class_num, class_num, image_size, image_size)\n",
    "print(vecs)\n",
    "print(fills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "# device : cpu or cuda:0/1/2/3\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "# G and D model\n",
    "G = DCGenerator(image_size=image_size, latent_dim=latent_dim, output_channel=image_channel, class_num=class_num)\n",
    "D = DCDiscriminator(image_size=image_size, input_channel=image_channel, class_num=class_num)\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "# G and D optimizer, use Adam or SGD\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "d_loss_hist, g_loss_hist = run_gan(trainloader, G, D, G_optimizer, D_optimizer, bceloss, \n",
    "                                   n_epochs, device, latent_dim, class_num)\n",
    "loss_plot(d_loss_hist, g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "可以观察到网络的结果是比较差的，因为类别标签是随机生成的，这导致生成器生成的假图是很容易被判别器正确识别。loss曲线中，G的loss值上升的速率较快，基本不会在固定的范围内波动，而D的loss值也有下降的趋势，可以看出该生成网络的效果是不如前面三次训练所得到的网络。\n",
    "\n",
    "这大概是因为生成器的生成结果与类别标签的关系随机性强，判别器因此更加容易判断该图为假图，而生成网络得到的调整弱，因此能力较前面三次训练弱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-image translation\n",
    "下面介绍一个使用CGAN来做Image-to-Image Translation的模型--pix2pix。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验使用的是Facade数据集，由于数据集的特殊性，一张图片包括两部分，如下图，左半边为groundtruth，右半边为轮廓，我们需要重写数据集的读取类，下面这个cell是就是用来读取数据集。最终使得我们的模型可以从右边部分的轮廓生成左边的建筑.\n",
    "![gan_d](./pictures/show.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*可以跳过阅读*)下面是dataset部分代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
    "        self.transform = transforms_\n",
    "        # read image\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # crop image,the left half if groundtruth image, and the right half is outline of groundtruth.\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_B = img.crop((0, 0, w / 2, h))\n",
    "        img_A = img.crop((w / 2, 0, w, h))\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            # revese the image by 50%\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transform(img_B)\n",
    "\n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成网络G,一个Encoder-Decoder模型，借鉴了U-Net结构，所谓的U-Net是将第i层拼接到第n-i层，这样做是因为第i层和第n-i层的图像大小是一致的。\n",
    "判别网络D,Pix2Pix中的D被实现为Patch-D，所谓Patch，是指无论生成的图像有多大，将其切分为多个固定大小的Patch输入进D去判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "##############################\n",
    "#           U-NET\n",
    "##############################\n",
    "\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            # when baych-size is 1, BN is replaced by instance normalization\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            # when baych-size is 1, BN is replaced by instance normalization\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 256, dropout=0.5)\n",
    "        self.down5 = UNetDown(256, 256, dropout=0.5)\n",
    "        self.down6 = UNetDown(256, 256, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(256, 256, dropout=0.5)\n",
    "        self.up2 = UNetUp(512, 256)\n",
    "        self.up3 = UNetUp(512, 256)\n",
    "        self.up4 = UNetUp(512, 128)\n",
    "        self.up5 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        d1 = self.down1(x)# 32x32\n",
    "        d2 = self.down2(d1)#16x16\n",
    "        d3 = self.down3(d2)#8x8\n",
    "        d4 = self.down4(d3)#4x4\n",
    "        d5 = self.down5(d4)#2x2\n",
    "        d6 = self.down6(d5)#1x1\n",
    "        u1 = self.up1(d6, d5)#2x2\n",
    "        u2 = self.up2(u1, d4)#4x4\n",
    "        u3 = self.up3(u2, d3)#8x8\n",
    "        u4 = self.up4(u3, d2)#16x16\n",
    "        u5 = self.up5(u4, d1)#32x32\n",
    "\n",
    "        return self.final(u5)#64x64\n",
    "\n",
    "\n",
    "##############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                # when baych-size is 1, BN is replaced by instance normalization\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False),#32x32\n",
    "            *discriminator_block(64, 128),#16x16\n",
    "            *discriminator_block(128, 256),#8x8\n",
    "            *discriminator_block(256, 256),#4x4\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(256, 1, 4, padding=1, bias=False)#4x4\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*可以跳过阅读*)下面这个函数用来保存轮廓图，生成图片，groundtruth，以作对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show\n",
    "def sample_images(dataloader, G, device):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    real_A = imgs[\"A\"].to(device)\n",
    "    real_B = imgs[\"B\"].to(device)\n",
    "    fake_B = G(real_A)\n",
    "    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "    show(torchvision.utils.make_grid(img_sample.cpu().data, nrow=5, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着定义一些超参数lambda_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper param\n",
    "n_epochs = 200\n",
    "batch_size = 2\n",
    "lr = 0.0002\n",
    "img_size = 64\n",
    "channels = 3\n",
    "device = torch.device('cuda:2')\n",
    "betas = (0.5, 0.999)\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于pix2pix的loss function，包括CGAN的loss，加上L1Loss，其中L1Loss之前有一个系数lambda，用于调节两者之间的权重。\n",
    "![gan_d](./pictures/loss_function.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里定义损失函数和优化器,这里损失函数使用了MSEloss作为GAN的loss(LSGAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import weights_init_normal\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_pixelwise = torch.nn.L1Loss().to(device)\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_size // 16, img_size // 16)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G = GeneratorUNet().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "# Configure dataloaders\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(\"./data/facades\", transforms_=transforms_),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\"./data/facades\", transforms_=transforms_, mode=\"val\"),\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始训练pix2pix,训练的过程:\n",
    "\n",
    "1. 首先训练G,对于每张图片A(轮廓),用G生成fakeB(建筑),然后fakeB与realB(ground truth)计算L1loss,同时使用D判别(fakeB,A),计算MSEloss(label为1),用这2个loss一起更新G;\n",
    "2. 再训练D,使用(fakeB,A)与(realB,A)计算MSEloss(label前者为0,后者为1),更新D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        #  G:B -> A\n",
    "        real_A = batch[\"A\"].to(device)\n",
    "        real_B = batch[\"B\"].to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real_label = torch.ones((real_A.size(0), *patch)).to(device)\n",
    "        fake_label = torch.zeros((real_A.size(0), *patch)).to(device)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = G(real_A)\n",
    "        pred_fake = D(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, real_label)\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, real_label)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = D(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, fake_label)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    # Print log\n",
    "    print(\n",
    "        \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f]\"\n",
    "        % (\n",
    "            epoch,\n",
    "            n_epochs,\n",
    "            i,\n",
    "            len(dataloader),\n",
    "            loss_D.item(),\n",
    "            loss_G.item(),\n",
    "            loss_pixel.item(),\n",
    "            loss_GAN.item(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # If at sample interval save image\n",
    "    if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "        sample_images(val_dataloader, G, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业**：\n",
    "\n",
    "1. 只用L1 Loss的情况下训练pix2pix.说说有结果什么不同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_pixelwise = torch.nn.L1Loss().to(device)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G = GeneratorUNet().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        #  G:B -> A\n",
    "        real_A = batch[\"A\"].to(device)\n",
    "        real_B = batch[\"B\"].to(device)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = G(real_A)\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "\n",
    "    # Print log\n",
    "    print(\n",
    "        \"\\r[Epoch %d/%d] [Batch %d/%d] [G loss: %f]\"\n",
    "        % (\n",
    "            epoch,\n",
    "            n_epochs,\n",
    "            i,\n",
    "            len(dataloader),\n",
    "            loss_G.item()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # If at sample interval save image\n",
    "    if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "        sample_images(val_dataloader, G, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "只用L1 loss训练网络的时候，可以观察到网络在一开始并没有像第一次训练得到的结果那样具有各种五颜六色的噪点。一开始的几次迭代可以很迅速地得到建筑的边框痕迹，相对第一次训练的噪点要少很多。但是若干次迭代之后，只用L1 loss的网络生成的假图很模糊，效果比第一次训练的结果差很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 只用CGAN Loss训练pix2pix(在下面的cell填入对应代码并运行).说说有结果什么不同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G = GeneratorUNet().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \"\"\"\n",
    "        To Do\n",
    "        \"\"\"\n",
    "        #  G:B -> A\n",
    "        real_A = batch[\"A\"].to(device)\n",
    "        real_B = batch[\"B\"].to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real_label = torch.ones((real_A.size(0), *patch)).to(device)\n",
    "        fake_label = torch.zeros((real_A.size(0), *patch)).to(device)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = G(real_A)\n",
    "        pred_fake = D(fake_B, real_A)\n",
    "        loss_G = criterion_GAN(pred_fake, real_label)\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D(real_B, real_A)\n",
    "        loss_real = criterion_GAN(pred_real, real_label)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = D(fake_B.detach(), real_A)\n",
    "        loss_fake = criterion_GAN(pred_fake, fake_label)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "\n",
    "    # Print log\n",
    "    print(\n",
    "        \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "        % (\n",
    "            epoch,\n",
    "            n_epochs,\n",
    "            i,\n",
    "            len(dataloader),\n",
    "            loss_D.item(),\n",
    "            loss_G.item()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # If at sample interval save image\n",
    "    if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "        sample_images(val_dataloader, G, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "只用CGAN Loss训练网络，生成的网络五颜六色的噪点非常多，比第一次训练所生成的图片还多。并且开始的几十代基本上没有办法得到一个建筑的基本纹理边框，而前两次训练的结果都能很快得到建筑的大致图像，虽然第一次训练也很慢。\n",
    "\n",
    "比较最终的结果，只用CGAN Loss的结果也是不如第一次训练所生成的假图，而只用CGAN Loss和只用L1 Loss的结果相差不大。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
